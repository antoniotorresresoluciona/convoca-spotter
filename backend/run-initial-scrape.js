import db from './database.js';
import { scrapeUrl, extractRelevantLinks } from './scraper.js';

console.log('ğŸš€ Iniciando scrape inicial de fuentes...\n');

// Obtener todas las fundaciones
const fundaciones = db.prepare('SELECT * FROM fundaciones WHERE enabled = 1').all();

console.log(`ğŸ“Š ${fundaciones.length} fundaciones para procesar\n`);

let processed = 0;
let totalSublinks = 0;

for (const fundacion of fundaciones) {
  console.log(`\n${'='.repeat(60)}`);
  console.log(`ğŸ›ï¸  ${fundacion.name}`);
  console.log(`ğŸ”— ${fundacion.url}`);
  console.log(`${'='.repeat(60)}`);

  try {
    // Scrape de la pÃ¡gina principal
    const { html, hash, sublinks, success } = await scrapeUrl(fundacion.url);

    if (!success) {
      console.log('  âŒ Error al scrapear');
      continue;
    }

    // Actualizar hash en la base de datos
    db.prepare(`
      UPDATE fundaciones
      SET last_hash = ?, last_checked = CURRENT_TIMESTAMP, status = 'checked'
      WHERE id = ?
    `).run(hash, fundacion.id);

    console.log(`  âœ“ PÃ¡gina scrapeada correctamente`);
    console.log(`  ğŸ“ ${sublinks.length} enlaces relevantes encontrados`);

    // Insertar sublinks
    const insertSublink = db.prepare(`
      INSERT OR IGNORE INTO sublinks (fundacion_id, url, link_text, depth)
      VALUES (?, ?, ?, 1)
    `);

    let added = 0;
    for (const sublink of sublinks.slice(0, 10)) { // Limitar a 10 por fundaciÃ³n
      try {
        insertSublink.run(fundacion.id, sublink.url, sublink.text);
        added++;
      } catch (err) {
        // Ignorar duplicados
      }
    }

    console.log(`  âœ¨ ${added} nuevos sublinks aÃ±adidos`);
    totalSublinks += added;
    processed++;

    // PequeÃ±a pausa entre scrapes
    await new Promise(resolve => setTimeout(resolve, 2000));

  } catch (error) {
    console.log(`  âŒ Error: ${error.message}`);
  }
}

console.log(`\n${'='.repeat(60)}`);
console.log('ğŸ“Š RESUMEN');
console.log(`${'='.repeat(60)}`);
console.log(`Fundaciones procesadas: ${processed}/${fundaciones.length}`);
console.log(`Total sublinks descubiertos: ${totalSublinks}`);
console.log('\nâœ… Scrape inicial completado!');
console.log('\nğŸ’¡ PrÃ³ximos pasos:');
console.log('   1. Los sublinks se crawlearÃ¡n automÃ¡ticamente por el cron job');
console.log('   2. O puedes ejecutar: npm run crawl-sublinks');
